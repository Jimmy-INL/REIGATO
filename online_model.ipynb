{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM-GenData-online_new.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "UDfLZnGiwk5R",
        "pihvljofwoEt",
        "L5bz5WQzPlIw",
        "qh9nLAskwv4c",
        "KwJp1J_r4_Bg",
        "zaveotlaPaMP"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O8KIU_yCYnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "import matplotlib as mpl\n",
        "drive.mount('/content/drive')\n",
        "kongpath = 'drive/My Drive/kongkat/'\n",
        "mpl.rcParams.update(mpl.rcParamsDefault)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpTDArwDDRjJ",
        "colab_type": "text"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDfLZnGiwk5R",
        "colab_type": "text"
      },
      "source": [
        "## Model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZA_SZRs4V7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class VRASAM(nn.Module):\n",
        "    def __init__(self, z_dim, T, x_dim, h_dim, batch_size=1, x_noise_factor=0.1):\n",
        "        super(VRASAM, self).__init__()\n",
        "\n",
        "        self.x_noise_factor = x_noise_factor\n",
        "        self.z_dim = z_dim\n",
        "        self.T = T\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.init_hidden()\n",
        "\n",
        "        # Everything going on in the network has to be of size:\n",
        "        # (batch_size, T, n_features)\n",
        "\n",
        "        # We encode the data onto the latent space using bi-directional LSTM\n",
        "        self.LSTM_encoder = nn.LSTM(\n",
        "            input_size=self.x_dim,\n",
        "            hidden_size=self.h_dim,\n",
        "            num_layers=1,\n",
        "            bias=True,\n",
        "            batch_first=True,\n",
        "            dropout=0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.fnn_zmu = nn.Linear(\n",
        "            in_features=2*self.h_dim,\n",
        "            out_features=self.z_dim,\n",
        "            bias=True\n",
        "        )\n",
        "\n",
        "        self.fnn_zvar = nn.Linear(\n",
        "            in_features=2*self.h_dim,\n",
        "            out_features=self.z_dim,\n",
        "            bias=True\n",
        "        )\n",
        "\n",
        "        self.fnn_cmu = nn.Linear(\n",
        "            in_features=2*self.h_dim,\n",
        "            out_features=self.z_dim,\n",
        "            bias=True\n",
        "        )\n",
        "\n",
        "        self.fnn_cvar = nn.Linear(\n",
        "            in_features=2*self.h_dim,\n",
        "            out_features=self.z_dim,\n",
        "            bias=True\n",
        "        )\n",
        "\n",
        "        # The latent code must be decoded into the original image\n",
        "        self.LSTM_decoder = nn.LSTM(\n",
        "            input_size=self.z_dim*2,\n",
        "            hidden_size=self.h_dim,\n",
        "            num_layers=1,\n",
        "            bias=True,\n",
        "            batch_first=True,\n",
        "            dropout=0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.fnn_xmu = nn.Linear(\n",
        "            in_features=2*self.h_dim,\n",
        "            out_features=self.x_dim,\n",
        "            bias=True\n",
        "        )\n",
        "\n",
        "        self.fnn_xb = nn.Linear(\n",
        "            in_features=2*self.h_dim,\n",
        "            out_features=self.x_dim,\n",
        "            bias=True\n",
        "        )\n",
        "\n",
        "    def init_hidden(self):\n",
        "        self.hidden_state = torch.zeros(2, self.batch_size, self.h_dim)\n",
        "        self.cell_state = torch.zeros(2, self.batch_size, self.h_dim)\n",
        "        if torch.cuda.is_available():\n",
        "            self.hidden_state = self.hidden_state.to(torch.device(0))\n",
        "            self.cell_state = self.cell_state.to(torch.device(0))\n",
        "\n",
        "    def encoder(self, x):\n",
        "        out_encoded, (hidden_T, _) = self.LSTM_encoder(\n",
        "            x, (self.hidden_state, self.cell_state))\n",
        "\n",
        "        # Swap batch dimension to batch_first\n",
        "        hidden_T = hidden_T.transpose(0, 1)\n",
        "\n",
        "        # Flatten \n",
        "        flat_hidden_T = hidden_T.reshape(self.batch_size, 1, 2*self.h_dim)\n",
        "        \n",
        "        # Fully connected layer from LSTM to var and mu\n",
        "        mu_z = self.fnn_zmu(flat_hidden_T)\n",
        "        sigma_z = nn.functional.softplus(self.fnn_zvar(flat_hidden_T))\n",
        "\n",
        "        # Calculate the similarity matrix\n",
        "        S = torch.matmul(\n",
        "            out_encoded,\n",
        "            torch.transpose(out_encoded, 1, 2)\n",
        "        )\n",
        "        \n",
        "        S = S / np.sqrt((2 * self.h_dim))\n",
        "\n",
        "        # Use softmax to get the sum of weights to equal 1\n",
        "        A = nn.functional.softmax(S, dim=2)\n",
        "        Cdet = torch.matmul(A, out_encoded)\n",
        "\n",
        "        # Fully connected layer from LSTM to var and mu\n",
        "        mu_c = self.fnn_cmu(Cdet)\n",
        "        sigma_c = nn.functional.softplus(self.fnn_cvar(Cdet))\n",
        "\n",
        "        return mu_z, sigma_z, mu_c, sigma_c\n",
        "\n",
        "    def decoder(self, c, z):\n",
        "        # Concatenate z and c before giving it as input to the decoder\n",
        "        z_cat = torch.cat(self.T*[z], dim=1)\n",
        "        zc_concat = torch.cat((z_cat, c), dim=2)\n",
        "        \n",
        "        # Run through decoder\n",
        "        out_decoded, _ = self.LSTM_decoder(zc_concat)\n",
        "\n",
        "        # Pass the decoder outputs through fnn to get LaPlace parameters\n",
        "        mu_x = self.fnn_xmu(out_decoded)\n",
        "        b_x = nn.functional.softplus(self.fnn_xb(out_decoded))\n",
        "\n",
        "        return mu_x, b_x\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "          var_x = self.x_noise_factor*torch.var(x,dim=1) # (batch_size, x_dim)\n",
        "          noise = torch.randn(self.T, self.batch_size, self.x_dim)\n",
        "\n",
        "          # (48, 16, 1) * (16, 1)\n",
        "          if torch.cuda.is_available():\n",
        "            noise = noise.to(torch.device(0))\n",
        "\n",
        "          \n",
        "          x_noise = torch.mul(noise, torch.sqrt(var_x)) # (T, batch_size, x_dim)\n",
        "          x = x + torch.transpose(x_noise, 0, 1) # (batch_size, T, x_dim)\n",
        "\n",
        "\n",
        "        # TODO: Add monte-carlo integration\n",
        "        outputs = {}\n",
        "        \n",
        "        mu_z, sigma_z, mu_c, sigma_c = self.encoder(x)\n",
        "        \n",
        "        # Don't propagate gradients through randomness\n",
        "        with torch.no_grad():\n",
        "            epsilon_z = torch.randn(self.batch_size, 1, self.z_dim)\n",
        "            epsilon_c = torch.randn(self.batch_size, self.T, self.z_dim)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "          epsilon_z = epsilon_z.to(torch.device(0))\n",
        "          epsilon_c = epsilon_c.to(torch.device(0))\n",
        "\n",
        "        z = mu_z + epsilon_z * sigma_z\n",
        "        c = mu_c + epsilon_c * sigma_c\n",
        "\n",
        "        mu_x, b_x = self.decoder(c, z)\n",
        "\n",
        "        outputs[\"z\"] = z\n",
        "        outputs[\"mu_z\"] = mu_z\n",
        "        outputs[\"sigma_z\"] = sigma_z\n",
        "        outputs[\"c\"] = c\n",
        "        outputs[\"mu_c\"] = mu_c\n",
        "        outputs[\"sigma_c\"] = sigma_c\n",
        "        outputs[\"mu_x\"] = mu_x\n",
        "        outputs[\"b_x\"] = b_x\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def count_parameters(self):\n",
        "        n_grad = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        n_total = sum(p.numel() for p in self.parameters())\n",
        "        return n_grad, n_total\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pihvljofwoEt",
        "colab_type": "text"
      },
      "source": [
        "## Loss and ReconProb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzHhR4Tfwg0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ELBO_loss(x, outputs, lambda_KL=0.01, eta_KL=0.01):\n",
        "    mu_x = outputs['mu_x']\n",
        "    b_x = outputs['b_x']\n",
        "    mu_z = outputs[\"mu_z\"]\n",
        "    sigma_z = outputs[\"sigma_z\"]\n",
        "    mu_c = outputs[\"mu_c\"]\n",
        "    sigma_c = outputs[\"sigma_c\"]\n",
        "\n",
        "    # Initialize Laplace with given parameters.\n",
        "    pdf_laplace = torch.distributions.laplace.Laplace(mu_x, b_x)\n",
        "    # Calculate mean of likelihood over T and x-dimension.\n",
        "    likelihood = pdf_laplace.log_prob(x).mean(dim=1).mean(dim=1)\n",
        "\n",
        "    # Calculate KL-divergence of p(c) and q(z)\n",
        "    v_z = sigma_z**2  # Variance of q(z)\n",
        "    v_c = sigma_c**2  # Variance of p(c)\n",
        "    kl_z = -0.5 * torch.sum(1 + torch.log(v_z) - mu_z**2 - v_z, dim=2)\n",
        "    kl_c = -0.5 * torch.sum(1 + torch.log(v_c) - mu_c**2 - v_c, dim=2)\n",
        "\n",
        "    kl_c = torch.sum(kl_c, dim=1)  # Sum over the T dimension.\n",
        "    kl_z = kl_z[:, 0]  # Get rid of extra x_dim dimension.\n",
        "\n",
        "    # Calculate the Evidence Lower Bound (ELBO)\n",
        "    ELBO = -likelihood + lambda_KL * (kl_z + eta_KL * kl_c)\n",
        "\n",
        "    # Return mean over all examples in batch\n",
        "    # =================\n",
        "    # ====== $ ===== RETURN: SUM instead of MEAN?\n",
        "    # =================\n",
        "    return torch.mean(ELBO, dim=0)\n",
        "\n",
        "\n",
        "def similarity_score(net, x, L):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Pass sequence through encoder to get params in q(z) and p(c)\n",
        "        mu_z, sigma_z, mu_c, sigma_c = net.encoder(x)\n",
        "        \n",
        "        score = 0\n",
        "\n",
        "        for _ in range(L):\n",
        "\n",
        "            # Sample a random c and z vector and reparametrize\n",
        "            epsilon_c = torch.randn(net.batch_size, net.T, net.z_dim)\n",
        "            epsilon_z = torch.randn(net.batch_size, 1, net.z_dim)\n",
        "            \n",
        "            if torch.cuda.is_available():\n",
        "                epsilon_c = epsilon_c.to(torch.device(0))\n",
        "                epsilon_z = epsilon_z.to(torch.device(0))\n",
        "\n",
        "            c = mu_c + epsilon_c * sigma_c\n",
        "            z = mu_z + epsilon_z * sigma_z\n",
        "\n",
        "            # Pass sample through decoder and calculate reconstruction prob\n",
        "            mu_x, b_x = net.decoder(c, z)\n",
        "            pdf_laplace = torch.distributions.laplace.Laplace(mu_x, b_x)\n",
        "            score += pdf_laplace.log_prob(x)\n",
        "\n",
        "        # Average over number of iterations\n",
        "        return score/L\n",
        "\n",
        "def get_sim_scores(net, dataset, L):\n",
        "\n",
        "    batch_size = min((len(dataset), dataset.T*100))\n",
        "\n",
        "    loader = DataLoader(dataset, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "    # Re-initialize network\n",
        "    net.batch_size = batch_size\n",
        "    net.init_hidden()\n",
        "    net.eval()\n",
        "    all_sim_scores = np.zeros((0, dataset.T))\n",
        "\n",
        "    for x, label in loader:\n",
        "        # Modify batch size and hidden state\n",
        "        if x.shape[0] != batch_size:\n",
        "            net.batch_size = x.shape[0]\n",
        "            net.init_hidden()\n",
        "\n",
        "        # Cast to gpu if available\n",
        "        if torch.cuda.is_available():\n",
        "            x = x.to(torch.device(0))\n",
        "        \n",
        "        temp_sim = tensor2numpy(similarity_score(net, x, L)[:, -1, 0]).reshape(-1, dataset.T)\n",
        "        all_sim_scores = np.concatenate((all_sim_scores, temp_sim))\n",
        "\n",
        "    return all_sim_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5bz5WQzPlIw",
        "colab_type": "text"
      },
      "source": [
        "## Online training and validation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53hhR8ECPoFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train(dataset, net, optimizer, criterion, batch_size, lamba_kl=0.01, num_workers=0):\n",
        "    \n",
        "    # Set network into training mode\n",
        "    net.batch_size = batch_size\n",
        "    net.init_hidden()\n",
        "    net.train()\n",
        "    \n",
        "    loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "    for x, label in loader:\n",
        "        \n",
        "        # Cast to gpu if available\n",
        "        if torch.cuda.is_available():\n",
        "            x = x.to(torch.device(0))\n",
        "        \n",
        "        # Change batch size in last iteration\n",
        "        if not x.shape[0] == batch_size:\n",
        "            net.batch_size = x.shape[0]\n",
        "            net.init_hidden()\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Make forward pass with model\n",
        "        outputs = net(x)\n",
        "        # Calculate loss and backprop\n",
        "        loss = criterion(x, outputs, 0.01, lamba_kl)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Revert to correct net batch size\n",
        "    net.batch_size = batch_size\n",
        "    net.init_hidden()\n",
        "\n",
        "    return net\n",
        "\n",
        "def validate(dataset, net, criterion, batch_size, lambda_kl=0.01):\n",
        "    \n",
        "    # Set network into evalution mode\n",
        "    net.batch_size = batch_size\n",
        "    net.init_hidden()\n",
        "    net.eval()\n",
        "\n",
        "    loader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, label in loader:\n",
        "        \n",
        "        # Cast to gpu if available\n",
        "        if torch.cuda.is_available():\n",
        "            x = x.to(torch.device(0))\n",
        "\n",
        "        # Change batch size in last iteration\n",
        "        if not x.shape[0] == batch_size:\n",
        "            net.batch_size = x.shape[0]\n",
        "            net.init_hidden()\n",
        "\n",
        "        # Make forward pass with model\n",
        "        outputs = net(x)\n",
        "        loss = criterion(x, outputs, 0.01, lambda_kl)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Set model to train mode and revert batch size\n",
        "    net.batch_size = batch_size\n",
        "    net.init_hidden()\n",
        "    net.train()\n",
        "\n",
        "    return total_loss/len(loader)\n",
        "\n",
        "def train_validate(dataset, net, optimizer, criterion, n_epochs, batch_size, train_val_split=(0.8, 0.2), num_workers=0, verbose=True):\n",
        "\n",
        "    train_size = int(train_val_split[0] * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    lambda_kl = 0\n",
        "    inc_val = 2/n_epochs\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "\n",
        "        # Train model\n",
        "        net = train(train_set, net, optimizer, criterion, batch_size, lambda_kl, num_workers)\n",
        "\n",
        "        # Evaluate model on training data again and validate\n",
        "        with torch.no_grad():\n",
        "            train_loss = validate(train_set, net, criterion, batch_size, lambda_kl)\n",
        "            val_loss = validate(val_set, net, criterion, batch_size, lambda_kl)\n",
        "\n",
        "        if verbose:\n",
        "            print(\n",
        "                \"Epoch {0}/{1} done!\\tTrain loss = {2:.2f}\\tVal loss = {3:.2f}\".format(\n",
        "                    e+1, n_epochs, train_loss, val_loss)\n",
        "            )\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        lambda_kl += inc_val\n",
        "        \n",
        "    return net, (train_losses, val_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh9nLAskwv4c",
        "colab_type": "text"
      },
      "source": [
        "## Misc function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJAlzuNnwjpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_z_values(net, dataset, batch_size, mu=False):\n",
        "\n",
        "    z_str = 'mu_z' if mu else 'z'\n",
        "\n",
        "    # Set batch size to length of dataset    \n",
        "    net.batch_size = batch_size\n",
        "    net.init_hidden()\n",
        "    net.eval()\n",
        "\n",
        "    loader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
        "\n",
        "    z_all = np.zeros((0, net.z_dim))\n",
        "    with torch.no_grad():\n",
        "        for x, label in loader:\n",
        "            if torch.cuda.is_available():\n",
        "                x = x.to(torch.device(0))\n",
        "            \n",
        "            # Change batch size in last iteration\n",
        "            if not x.shape[0] == batch_size:\n",
        "                net.batch_size = x.shape[0]\n",
        "                net.init_hidden()\n",
        "                \n",
        "            output = net(x)\n",
        "            \n",
        "            # Extract z\n",
        "            z = output[z_str]\n",
        "            z_all = np.concatenate((z_all, tensor2numpy(z[:,0])))\n",
        "\n",
        "    # Restore batch_size\n",
        "    net.batch_size = batch_size\n",
        "    net.init_hidden()\n",
        "\n",
        "    return z_all\n",
        "\n",
        "def tensor2numpy(x):\n",
        "    if x.requires_grad:\n",
        "        x = torch.Tensor.cpu(x).detach().numpy()\n",
        "    else:\n",
        "        x = torch.Tensor.cpu(x).numpy()\n",
        "    return x\n",
        "\n",
        "def save_model(net, name):\n",
        "    _drive = \"/content/drive/My Drive/kongkat/models\"\n",
        "    f_name = name + \".pth\"\n",
        "    f_path = os.path.join(_drive, f_name)\n",
        "    torch.save(net.state_dict(), f_path)\n",
        "\n",
        "def load_model(parameters, name):\n",
        "    _drive = \"/content/drive/My Drive/kongkat/models\"\n",
        "    f_name = name + \".pth\"\n",
        "    f_path = os.path.join(_drive, f_name)\n",
        "    \n",
        "    model = VRASAM(*parameters)\n",
        "    model.load_state_dict(torch.load(f_path))\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.to(torch.device(0))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwJp1J_r4_Bg",
        "colab_type": "text"
      },
      "source": [
        "# GENDATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iQZkBq-5Ek4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class GENDATA_TRAIN(Dataset):\n",
        "\n",
        "    def __init__(self, N_seq, T_w):\n",
        "\n",
        "        datafolder = os.path.normpath(\"drive/My Drive/kongkat/data/GENDATA/\")\n",
        "        path = os.path.normpath(f\"{datafolder}/train_data.txt\")\n",
        "        self.data = np.genfromtxt(path)\n",
        "        self.N_seq = N_seq\n",
        "        self.T = 96\n",
        "        self.data = self.data[:(self.N_seq*self.T)] # Extract wanted number of sequences\n",
        "        self.T_w = T_w # Sliding window length\n",
        "        self.N_obs = self.data.shape[0] # Total number of observations\n",
        "        self.N_seq = int(self.N_obs / self.T) # Total number of sequences\n",
        "        self.N_w = self.N_obs - self.T_w + 1  # Total number of windows\n",
        "        self.X = torch.Tensor(np.zeros((self.N_w, self.T_w)))\n",
        "        \n",
        "        # Define labels for sorted training and test data\n",
        "        self.time_labels = (np.arange(self.T_w-1, (self.T_w+self.N_w)-1) % self.T) + 1\n",
        "        \n",
        "        # Sliding window loop\n",
        "        for w_start in range(self.N_w):\n",
        "            # Extract window\n",
        "            self.X[w_start] = torch.Tensor(self.data[w_start:(w_start+T_w)])\n",
        "\n",
        "        # Assertions\n",
        "        assert len(self.time_labels) == len(self.X), \"labels and X are not the same size\"\n",
        "        assert N_seq <= int(self.N_obs / self.T), \"Tried to load more data than available\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        x = x.view(*x.shape, 1)\n",
        "        return x, self.time_labels[idx]\n",
        "\n",
        "    def get_window(self, idx):\n",
        "        x = self.X[idx]\n",
        "        x = x.view(1, -1, 1)\n",
        "        return x\n",
        "\n",
        "class GENDATA_VAL(Dataset):\n",
        "\n",
        "    def __init__(self, T_w):\n",
        "\n",
        "        datafolder = os.path.normpath(\"drive/My Drive/kongkat/data/GENDATA/\")\n",
        "        data_path = os.path.normpath(f\"{datafolder}/val_data.txt\")\n",
        "        anom_path = os.path.normpath(f\"{datafolder}/val_anoms.txt\")\n",
        "        label_path = os.path.normpath(f\"{datafolder}/val_labels.txt\")\n",
        "\n",
        "        start_idx = 96 - T_w + 1 # Skip first sequence \n",
        "        self.data = np.genfromtxt(data_path)[start_idx:]\n",
        "        self.anom_labels = np.genfromtxt(anom_path)[1:]\n",
        "        self.labels = np.genfromtxt(label_path, dtype=\"|U5\")[1:]\n",
        "        self.T = 96\n",
        "        self.T_w = T_w # Sliding window length\n",
        "        self.N_obs = self.data.shape[0] # Total number of observations\n",
        "        self.N_seq = int(self.N_obs / self.T) # Total number of sequences\n",
        "        self.N_w = self.N_obs - self.T_w + 1  # Total number of windows\n",
        "        self.X = torch.Tensor(np.zeros((self.N_w, self.T_w)))\n",
        "        \n",
        "        # Define labels for sorted training and val data\n",
        "        self.time_labels = (np.arange(self.T_w-1, (self.T_w+self.N_w)-1) % self.T) + 1\n",
        "        \n",
        "        # Sliding window loop\n",
        "        for w_start in range(self.N_w):\n",
        "            # Extract window\n",
        "            self.X[w_start] = torch.Tensor(self.data[w_start:(w_start+T_w)])\n",
        "\n",
        "        # Assertions\n",
        "        assert len(self.time_labels) == len(self.X), \"labels and X are not the same size\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        x = x.view(*x.shape, 1)\n",
        "        return x, self.time_labels[idx]\n",
        "\n",
        "    def get_window(self, idx):\n",
        "        x = self.X[idx]\n",
        "        x = x.view(1, -1, 1)\n",
        "        return x\n",
        "\n",
        "class GENDATA_TEST(Dataset):\n",
        "\n",
        "    def __init__(self, T_w):\n",
        "\n",
        "        datafolder = os.path.normpath(\"drive/My Drive/kongkat/data/GENDATA/\")\n",
        "        data_path = os.path.normpath(f\"{datafolder}/test_data.txt\")\n",
        "        anom_path = os.path.normpath(f\"{datafolder}/test_anoms.txt\")\n",
        "        label_path = os.path.normpath(f\"{datafolder}/test_labels.txt\")\n",
        "\n",
        "        start_idx = 96 - T_w + 1 # Skip first sequence \n",
        "        self.data = np.genfromtxt(data_path)[start_idx:]\n",
        "        self.anom_labels = np.genfromtxt(anom_path)[1:]\n",
        "        self.labels = np.genfromtxt(label_path, dtype=\"|U5\")[1:]\n",
        "        self.T = 96\n",
        "        self.T_w = T_w # Sliding window length\n",
        "        self.N_obs = self.data.shape[0] # Total number of observations\n",
        "        self.N_seq = int(self.N_obs / self.T) # Total number of sequences\n",
        "        self.N_w = self.N_obs - self.T_w + 1  # Total number of windows\n",
        "        self.X = torch.Tensor(np.zeros((self.N_w, self.T_w)))\n",
        "        \n",
        "        # Define labels for sorted training and test data\n",
        "        self.time_labels = (np.arange(self.T_w-1, (self.T_w+self.N_w)-1) % self.T) + 1\n",
        "        \n",
        "        # Sliding window loop\n",
        "        for w_start in range(self.N_w):\n",
        "            # Extract window\n",
        "            self.X[w_start] = torch.Tensor(self.data[w_start:(w_start+T_w)])\n",
        "\n",
        "        # Assertions\n",
        "        assert len(self.time_labels) == len(self.X), \"labels and X are not the same size\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        x = x.view(*x.shape, 1)\n",
        "        return x, self.time_labels[idx]\n",
        "\n",
        "    def get_window(self, idx):\n",
        "        x = self.X[idx]\n",
        "        x = x.view(1, -1, 1)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaveotlaPaMP",
        "colab_type": "text"
      },
      "source": [
        "# Results plot functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_faQuWeY9er",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.lines import Line2D\n",
        "import torch\n",
        "\n",
        "def plot_sim(ax, x, samples, sim, min_sim, max_sim, title):\n",
        "\n",
        "    ax[0].plot(tensor2numpy(x[:, -1, 0]), c='black')\n",
        "    ax[0].set_title(title)\n",
        "\n",
        "    # Plot regenerated normal data\n",
        "    ax[0].plot(samples, 'o', markersize=1, c='darksalmon', alpha=0.3, rasterized=True)\n",
        "    ax[0].set_xlim((0, 96))\n",
        "    ax[0].grid(False)\n",
        "    ax[0].set_xticks([])\n",
        "    ax[0].set_yticks([])\n",
        "\n",
        "    # Plot similarity\n",
        "    ax[1].imshow(tensor2numpy(sim).reshape(1, -1), aspect='auto', cmap='RdYlGn', vmin=min_sim, vmax=max_sim)\n",
        "    ax[1].grid(False)\n",
        "    ax[1].set_yticks([])\n",
        "    ax[1].set_xticks([])\n",
        "\n",
        "    return ax\n",
        "\n",
        "def results_plot(ax1, ax2, net, all_sim_scores, L=1, n_samples=10):\n",
        "\n",
        "    outlier_types = [\"Snow\", \"Fault\", \"Spike\", \"Shade\", \"Cloud\"]\n",
        "    \n",
        "    # Get min and max similarity score\n",
        "    min_sim = np.quantile(all_sim_scores, 0.02)\n",
        "    max_sim = np.quantile(all_sim_scores, 0.9)\n",
        "\n",
        "    # Load   data\n",
        "    test_data = GENDATA_TEST(T_w)\n",
        "    X = test_data.X.reshape(-1, test_data.T, test_data.T_w)\n",
        "    labels = test_data.labels\n",
        "\n",
        "    # Extract random normal sequence\n",
        "    normal_X = X[labels == \"None\"]\n",
        "    x_normal = random.choice(normal_X).unsqueeze(2)\n",
        "\n",
        "    # Allocate outliers array\n",
        "    X_outliers = torch.Tensor(np.zeros((len(outlier_types), *(x_normal.shape))))\n",
        "\n",
        "    # Extract specified or random outlier sequence\n",
        "    for i, out_type in enumerate(outlier_types):\n",
        "        if out_type == \"Shade\":\n",
        "            rand_outlier_idx = 134\n",
        "        else:\n",
        "            outlier_idxs = np.arange(X.shape[0])[labels == out_type]\n",
        "            rand_outlier_idx = random.choice(outlier_idxs)\n",
        "        X_outliers[i] = X[rand_outlier_idx].unsqueeze(2)\n",
        "\n",
        "    # Cast to GPU\n",
        "    if torch.cuda.is_available():\n",
        "        x_normal = x_normal.to(torch.device(0))\n",
        "        X_outliers = X_outliers.to(torch.device(0))\n",
        "    \n",
        "    # Reshape outliers data\n",
        "    X_outliers = X_outliers.view(-1, test_data.T_w, 1)\n",
        "\n",
        "    # Create regeneration\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Set network batch size down and reinitalize hidden- and cell state\n",
        "        net.batch_size = x_normal.shape[0]\n",
        "        net.init_hidden()\n",
        "\n",
        "        # Allocate arrays for normal samples\n",
        "        normal_samples = np.zeros((len(x_normal), n_samples))\n",
        "\n",
        "        # Calculate similarity\n",
        "        sim_normal = similarity_score(net, x_normal, L)[:, -1, 0]\n",
        "\n",
        "        # Pass sequences through encoder to get params in q(z) and p(c)\n",
        "        mu_z_normal, sigma_z_normal, mu_c_normal, sigma_c_normal  = net.encoder(x_normal)\n",
        "        \n",
        "        # Sample n_samples times\n",
        "        for i in range(n_samples):\n",
        "            # Sample a random c and z vector and reparametrize\n",
        "            epsilon_c_normal = torch.randn(net.batch_size, net.T, net.z_dim)\n",
        "            epsilon_z_normal = torch.randn(net.batch_size, 1, net.z_dim)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                epsilon_c_normal = epsilon_c_normal.to(torch.device(0))\n",
        "                epsilon_z_normal = epsilon_z_normal.to(torch.device(0))\n",
        "\n",
        "            c_normal = mu_c_normal + epsilon_c_normal * sigma_c_normal\n",
        "            z_normal = mu_z_normal + epsilon_z_normal * sigma_z_normal\n",
        "\n",
        "            # Pass sample through decoder and calculate reconstruction prob\n",
        "            mu_x_normal, b_x_normal = net.decoder(c_normal, z_normal)\n",
        "            normal_samples[:, i] = tensor2numpy(mu_x_normal[:, -1, 0])\n",
        "\n",
        "        # Set network batch size down and reinitalize hidden- and cell state\n",
        "        net.batch_size = X_outliers.shape[0]\n",
        "        net.init_hidden()\n",
        "\n",
        "        # Allocate arrays for outlier samples\n",
        "        outlier_samples = np.zeros((len(X_outliers),  n_samples))\n",
        "\n",
        "        sim_outlier = similarity_score(net, X_outliers, L=L)[:, -1, 0]\n",
        "        mu_z_outlier, sigma_z_outlier, mu_c_outlier, sigma_c_outlier = net.encoder(X_outliers)        \n",
        "\n",
        "        # Sample n_samples times\n",
        "        for i in range(n_samples):\n",
        "\n",
        "            epsilon_c_outlier = torch.randn(net.batch_size, net.T, net.z_dim)\n",
        "            epsilon_z_outlier = torch.randn(net.batch_size, 1, net.z_dim)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                epsilon_c_outlier = epsilon_c_outlier.to(torch.device(0))\n",
        "                epsilon_z_outlier = epsilon_z_outlier.to(torch.device(0))\n",
        "\n",
        "            c_outlier = mu_c_outlier + epsilon_c_outlier * sigma_c_outlier\n",
        "            z_outlier = mu_z_outlier + epsilon_z_outlier * sigma_z_outlier\n",
        "\n",
        "            mu_x_outlier, b_x_outlier = net.decoder(c_outlier, z_outlier)\n",
        "            outlier_samples[:, i] = tensor2numpy(mu_x_outlier[:, -1, 0])\n",
        "\n",
        "    # Reshape outlier similarity and samples\n",
        "    outlier_samples = outlier_samples.reshape(len(outlier_types), -1, n_samples)\n",
        "    sim_outlier = sim_outlier.view(len(outlier_types), -1, 1)\n",
        "    X_outliers = X_outliers.view(len(outlier_types), -1, test_data.T_w, 1)\n",
        "\n",
        "    # Define range of plotting\n",
        "    plot_lim = (-0.1, 1.1)\n",
        "\n",
        "    ## Plot outlier data ##\n",
        "    outlier_axes = [ax1[:, 1], ax1[:, 2], ax2[:, 0], ax2[:, 1], ax2[:, 2]]\n",
        "    for i in range(len(outlier_axes)):\n",
        "        plot_sim(outlier_axes[i], X_outliers[i], outlier_samples[i], sim_outlier[i], min_sim, max_sim, f'{outlier_types[i].capitalize()}')\n",
        "        outlier_axes[i][0].set_ylim(plot_lim)\n",
        "\n",
        "    ## Plot normal data ##\n",
        "    plot_sim(ax1[:, 0], x_normal, normal_samples, sim_normal, min_sim, max_sim, 'Normal data')\n",
        "    ax1[0, 0].set_ylim(plot_lim)\n",
        "    ax1[0, 0].set_ylabel('Energy', fontsize=14)\n",
        "    ax2[0, 0].set_ylabel('Energy', fontsize=14)\n",
        "    ax1[0, 0].set_yticks([0, 1])\n",
        "    ax2[0, 0].set_yticks([0, 1])\n",
        "    ax1[0, 0].yaxis.set_tick_params(labelsize=14)\n",
        "    ax2[0, 0].yaxis.set_tick_params(labelsize=14)\n",
        "\n",
        "    return ax1, ax2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7AJBFGqIg6o",
        "colab_type": "text"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZW370OTWqPD",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIrk0fTgptJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Set seed mu\n",
        "seed = 7\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "N = 1400  # Number of sequences\n",
        "T_w = 32  # Window length\n",
        "batch_size = 5000\n",
        "train_val_split = (0.8, 0.2)\n",
        "train_data = GENDATA_TRAIN(N, T_w)\n",
        "\n",
        "# Model hyperparameters\n",
        "n_epochs = 50\n",
        "z_dim = 3\n",
        "h_dim = 128  # Number of LSTM units in each direction\n",
        "x_dim = 1\n",
        "lr = 0.001\n",
        "criterion = ELBO_loss\n",
        "parameters = [z_dim, T_w, x_dim, h_dim, batch_size]\n",
        "net = VRASAM(*parameters)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr, amsgrad=True)\n",
        "\n",
        "# Train model\n",
        "if torch.cuda.is_available():\n",
        "    net = net.to(torch.device(0))\n",
        "\n",
        "pars = {'dataset': train_data, \n",
        "        'net': net,\n",
        "        'optimizer': optimizer,\n",
        "        'criterion': criterion,\n",
        "        'n_epochs': n_epochs,\n",
        "        'batch_size': batch_size,\n",
        "        'train_val_split': train_val_split,\n",
        "        'num_workers': 0,\n",
        "        'verbose': True}\n",
        "\n",
        "net, (train_losses, val_losses) = train_validate(**pars)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LDDCa_wWs8D",
        "colab_type": "text"
      },
      "source": [
        "## Vizualization of model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDgnlUuK8SnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = GENDATA_TEST(T_w)\n",
        "test_all_sim_scores = get_sim_scores(net, test_data, L=512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrlxADHuHjLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig1, ax1 = plt.subplots(2, 3, \n",
        "                       gridspec_kw={\n",
        "                           'width_ratios': [1, 1, 1],\n",
        "                           'height_ratios': [5, 1]},\n",
        "                       figsize=(10, 2))\n",
        "fig1.subplots_adjust(hspace=0, wspace=0.05)\n",
        "\n",
        "fig2, ax2 = plt.subplots(2, 3, \n",
        "                       gridspec_kw={\n",
        "                           'width_ratios': [1, 1, 1],\n",
        "                           'height_ratios': [5, 1]},\n",
        "                       figsize=(10, 2))\n",
        "fig2.subplots_adjust(hspace=0, wspace=0.05)\n",
        "\n",
        "# Visualize the reconstruction results on normal and outlier series\n",
        "ax1, ax2 = results_plot(ax1, ax2, net, test_all_sim_scores, L=512, n_samples=50)\n",
        "fig1.savefig(kongpath+'figures/regen_plot_online1.pdf', format='pdf')\n",
        "fig2.savefig(kongpath+'figures/regen_plot_online2.pdf', format='pdf')\n",
        "plt.show()\n",
        "\n",
        "#fig, ax = plt.subplots(1,1,figsize=(10, 3))\n",
        "#ax.plot(train_losses, label='Training loss', c='darkslategrey')\n",
        "#ax.plot(val_losses, label='Validation loss', c='darksalmon')\n",
        "#ax.set_xlabel('Epochs', fontsize=14)\n",
        "#ax.set_ylabel('Loss', fontsize=14)\n",
        "#plt.legend(fontsize=14)\n",
        "#plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6WLi33g9mhC",
        "colab_type": "text"
      },
      "source": [
        "# Model visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRciBaYy9YV0",
        "colab_type": "text"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLUYtB5moaUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "# Extract all z_values for training data and standardize\n",
        "z_all = get_z_values(net, train_data, batch_size=5000)\n",
        "z_all = StandardScaler().fit_transform(z_all)\n",
        "\n",
        "pca = PCA()\n",
        "principal_comps = pca.fit_transform(z_all)\n",
        "print(pca.explained_variance_ratio_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Tm3kZ7AuP7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1,1, figsize=(7,6))\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "\n",
        "scat = ax.scatter(principal_comps[:,0], principal_comps[:,1], s=1, c=train_data.time_labels, cmap='plasma_r', rasterized=True)\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.axis('off')\n",
        "cbar = fig.colorbar(scat, cax=cax, orientation='vertical')\n",
        "cbar.set_label('Time of day', fontsize=14)\n",
        "cbar.ax.tick_params(labelsize=14)\n",
        "ticks = [i*4 for i in range(0, 25, 3)]\n",
        "ticks[0] += 1\n",
        "tick_labels = [f\"{int(l/4):02}\" for l in ticks]\n",
        "cbar.set_ticks(ticks)\n",
        "cbar.set_ticklabels(tick_labels)\n",
        "plt.tight_layout()\n",
        "plt.savefig('drive/My Drive/kongkat/figures/beautiful_online_pca.pdf', format='pdf')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MinUMDADUHrD",
        "colab_type": "text"
      },
      "source": [
        "# Outlier visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGJrkNd2UK80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "# Process test data\n",
        "test_labels = test_data.labels\n",
        "test_anoms = test_data.anom_labels\n",
        "test_z_all = get_z_values(net, test_data, batch_size=5000, mu=True)\n",
        "\n",
        "# Extract all z_values for training data and standardize\n",
        "test_z_all = StandardScaler().fit_transform(test_z_all)\n",
        "\n",
        "pca = PCA()\n",
        "principal_comps = pca.fit_transform(test_z_all)\n",
        "principal_comps = principal_comps.reshape(len(test_labels), 96, z_dim)\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7BdPqGjUMwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_normals = 5\n",
        "normal_idxs = np.arange(test_labels.shape[0])[test_labels == \"None\"]\n",
        "plot_idxs = list(np.random.choice(normal_idxs, size=n_normals, replace=False))\n",
        "outlier_types = [\"Snow\", \"Fault\", \"Spike\", \"Shade\", \"Cloud\"]\n",
        "\n",
        "## Plot random outliers (comment for specific outliers below)\n",
        "#for out in outlier_types:\n",
        "#    outlier_idxs = np.arange(test_labels.shape[0])[test_labels == out]\n",
        "#    plot_idxs.append(np.random.choice(outlier_idxs))\n",
        "\n",
        "## Uncomment and insert chosen indices into dictionary once found ##\n",
        "chosen_ones = {'Snow': 1106,\n",
        "               'Fault': 546,\n",
        "               'Spike': 866,\n",
        "               'Shade': 568,\n",
        "               'Cloud': 436}\n",
        "plot_idxs.extend(list(chosen_ones.values()))\n",
        "\n",
        "exp_var = sum(pca.explained_variance_ratio_[:2])*100\n",
        "col2label = {\"None\": 'black',\n",
        "             \"Snow\": 'darksalmon', \n",
        "             \"Fault\": 'springgreen', \n",
        "             \"Spike\": 'skyblue', \n",
        "             \"Shade\": 'fuchsia', \n",
        "             \"Cloud\": 'orangered'}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dui0aUqN2bZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.lines import Line2D\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(7,6))\n",
        "\n",
        "for idx in plot_idxs:\n",
        "    lab = test_labels[idx]\n",
        "    if lab == \"None\":\n",
        "        comp = principal_comps[idx]\n",
        "        ax.plot(comp[:,0].T, comp[:,1].T, 'o-', c=col2label[lab], markersize=5, linewidth=1)\n",
        "    else:\n",
        "        comp = principal_comps[idx]\n",
        "        anom_start, anom_end = test_anoms[idx]\n",
        "        all_idxs = np.arange(96)\n",
        "        out_idxs = (all_idxs<=anom_end)*(all_idxs>=anom_start)\n",
        "        norm_idxs1 = all_idxs<=anom_start\n",
        "        norm_idxs2 = all_idxs>=anom_end\n",
        "        ax.plot(comp[norm_idxs1,0].T, comp[norm_idxs1,1].T, 'o-', c=col2label[\"None\"], markersize=5, linewidth=1)\n",
        "        ax.plot(comp[norm_idxs2,0].T, comp[norm_idxs2,1].T, 'o-', c=col2label[\"None\"], markersize=5, linewidth=1)\n",
        "        ax.plot(comp[out_idxs,0].T, comp[out_idxs,1].T, 'o-', c=col2label[lab], markersize=5, linewidth=1)\n",
        "\n",
        "lines = [Line2D([0], [0], color=c, linewidth=3, linestyle='-', marker='o') for k,c in col2label.items()]\n",
        "leg_labels = [name if name!=\"None\" else \"Normal\" for name in col2label.keys()]\n",
        "#leg_labels = [\"Normal\"] + [name+f\" (idx {plot_idxs[n_normals+i]})\" for i,name in enumerate(list(col2label.keys())[1:])]\n",
        "#ax.set_xlabel('Principal component 1')\n",
        "#ax.set_ylabel('Principal component 2')\n",
        "ax.tick_params(axis='both', which='major')\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.axis('off')\n",
        "plt.legend(lines, leg_labels, frameon=False, fontsize=14, loc=(-0.1,0))\n",
        "#plt.title(f'Normal- and outlier sequences in z-space ({exp_var:.2f}% explained variance)', fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('drive/My Drive/kongkat/figures/online_outlier_pca.pdf', format='pdf')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK8yRf8tLibq",
        "colab_type": "text"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiRf_nDKLkNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "def plotRocCurve(ax, ytrue, sim_scores):\n",
        "    #Extract propriate values for roc-curve\n",
        "    ytrue_roc = []\n",
        "    for i in range(len(ytrue)):\n",
        "      if ytrue[i] == 0:\n",
        "        ytrue_roc.append(1)\n",
        "      else:\n",
        "        ytrue_roc.append(0)\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(ytrue_roc, sim_scores)\n",
        "    auc = roc_auc_score(ytrue_roc, sim_scores)\n",
        "    random = [0,1]\n",
        "\n",
        "    # plot the roc curve for the model\n",
        "    ax.plot(fpr, tpr, linestyle='--', label=f'Threshold prediction (AUC={auc:.2f})', c='darkslategrey')\n",
        "    ax.plot(random, random, linestyle='--', label='No information', c='darksalmon')\n",
        "    # axis labels\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.tick_params(axis='both', which='major')\n",
        "    ax.set_xticks([0, 0.5, 1])\n",
        "    ax.set_yticks([0, 0.5, 1])\n",
        "    # show the legend\n",
        "    ax.legend()\n",
        "\n",
        "    return thresholds, auc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey686mywagPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sim_scores = test_all_sim_scores.sum(axis=1)\n",
        "test_ytrue = test_labels != \"None\"\n",
        "fig, ax = plt.subplots(1, 1, figsize=(5,3))\n",
        "thresholds, auc = plotRocCurve(ax, test_ytrue, test_sim_scores)\n",
        "plt.savefig(kongpath+'figures/ROC_online.pdf', format='pdf')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NypYnine1wna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load threshold data\n",
        "val_data = GENDATA_VAL(T_w)\n",
        "val_all_sim_scores = get_sim_scores(net, val_data, L=512)\n",
        "val_sim_scores = val_all_sim_scores.sum(axis=1)\n",
        "val_ytrue = val_data.labels != 'None'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-OkyGT1l1uD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(15,5))\n",
        "acc = np.zeros(len(thresholds))\n",
        "\n",
        "for i,thres in enumerate(thresholds):\n",
        "    val_ypred = val_sim_scores < thres\n",
        "    trues = sum(val_ypred == val_ytrue)\n",
        "    total = len(val_ypred)\n",
        "    acc[i] = trues/total\n",
        "\n",
        "ax.bar(range(len(thresholds)), acc, color='darkslategrey')\n",
        "ax.set_xticks([])\n",
        "ax.set_xlabel('Reconstruction likelihood thresholds', fontsize=16)\n",
        "ax.set_ylabel('Accuracy', fontsize=16)\n",
        "for index, value in enumerate(acc):\n",
        "    plt.text(index, value-0.05, f'{value*100:.0f}', ha='center', color='white', fontsize=10)\n",
        "plt.show()\n",
        "\n",
        "# Extract best threshold\n",
        "threshold = thresholds[np.argmax(acc)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4Zh5ikK6Kqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Process test data\n",
        "test_sim_scores = test_all_sim_scores.sum(axis=1)\n",
        "test_labels = test_data.labels\n",
        "test_ytrue = test_labels != 'None'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTksmRMIL02Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "\n",
        "class_names = [\"None\", \"Snow\", \"Fault\", \"Spike\", \"Shade\", \"Cloud\"]\n",
        "\n",
        "test_ypred = test_sim_scores < threshold\n",
        "correct = test_ytrue == test_ypred\n",
        "conf_mat = np.zeros((2, len(class_names)))\n",
        "\n",
        "total_dict = Counter(test_labels)\n",
        "\n",
        "for i,cls in enumerate(class_names):\n",
        "    acc_cls = sum(correct[test_labels == cls])/total_dict[cls]\n",
        "    if cls == \"None\":\n",
        "        conf_mat[0,i] = acc_cls\n",
        "        conf_mat[1,i] = 1 - acc_cls\n",
        "    else:\n",
        "        conf_mat[0,i] = 1 - acc_cls\n",
        "        conf_mat[1,i] = acc_cls\n",
        "\n",
        "class_names = [\"Normal\", \"Snow\", \"Fault\", \"Spike\", \"Shade\", \"Cloud\"]\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6,2))\n",
        "df_cm = pd.DataFrame(conf_mat, ['Normal', 'Outlier'], class_names)\n",
        "sns.set(font_scale=1.4)#for label size\n",
        "sns.heatmap(df_cm, fmt='.2f', annot=True, annot_kws={\"size\": 10}, ax=ax, cmap='Greys', cbar=False)# font size\n",
        "\n",
        "ax.set_yticklabels(['Normal', 'Outlier'], va='center')\n",
        "ax.set_xlabel(\"True value\", fontsize=12)\n",
        "ax.set_ylabel(\"Prediction\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(kongpath+'figures/confusion_matrix_online.pdf', format='pdf')\n",
        "plt.show()\n",
        "mpl.rcParams.update(mpl.rcParamsDefault)\n",
        "print(f'Final accuracy: {sum(correct)/len(correct):.2f}')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}