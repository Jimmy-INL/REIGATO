{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM-GenData-new.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "wpTDArwDDRjJ",
        "KwJp1J_r4_Bg",
        "JXufJlB5DX6J",
        "zaveotlaPaMP"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O8KIU_yCYnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "kongpath = 'drive/My Drive/kongkat/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpTDArwDDRjJ",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZA_SZRs4V7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class VRASAM(nn.Module):\n",
        "    def __init__(self, z_dim, T, x_dim, h_dim, batch_size=1, x_noise_factor=0.1):\n",
        "        super(VRASAM, self).__init__()\n",
        "\n",
        "        self.x_noise_factor = x_noise_factor\n",
        "        self.z_dim = z_dim\n",
        "        self.T = T\n",
        "        self.x_dim = x_dim\n",
        "        self.h_dim = h_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.init_hidden()\n",
        "\n",
        "        # Everything going on in the network has to be of size:\n",
        "        # (batch_size, T, n_features)\n",
        "\n",
        "        # We encode the data onto the latent space using bi-directional LSTM\n",
        "        self.LSTM_encoder = nn.LSTM(\n",
        "            input_size=self.x_dim,\n",
        "            hidden_size=self.h_dim,\n",
        "            num_layers=1,\n",
        "            bias=False,\n",
        "            batch_first=True,\n",
        "            dropout=0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.fnn_zmu = nn.Linear(\n",
        "            in_features=2*self.h_dim,\n",
        "            out_features=self.z_dim,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        self.fnn_zvar = nn.Linear(\n",
        "            in_features=2*self.h_dim,\n",
        "            out_features=self.z_dim,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        self.fnn_cmu = nn.Linear(\n",
        "            in_features=2*self.h_dim,\n",
        "            out_features=self.z_dim,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        self.fnn_cvar = nn.Linear(\n",
        "            in_features=2*self.h_dim,\n",
        "            out_features=self.z_dim,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        # The latent code must be decoded into the original image\n",
        "        self.LSTM_decoder = nn.LSTM(\n",
        "            input_size=self.z_dim*2,\n",
        "            hidden_size=self.h_dim,\n",
        "            num_layers=1,\n",
        "            bias=False,\n",
        "            batch_first=True,\n",
        "            dropout=0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.fnn_xmu = nn.Linear(\n",
        "            in_features=2*self.h_dim,\n",
        "            out_features=self.x_dim,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        self.fnn_xb = nn.Linear(\n",
        "            in_features=2*self.h_dim,\n",
        "            out_features=self.x_dim,\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "    def init_hidden(self):\n",
        "        self.hidden_state = torch.zeros(2, self.batch_size, self.h_dim)\n",
        "        self.cell_state = torch.zeros(2, self.batch_size, self.h_dim)\n",
        "        if torch.cuda.is_available():\n",
        "            self.hidden_state = self.hidden_state.to(torch.device(0))\n",
        "            self.cell_state = self.cell_state.to(torch.device(0))\n",
        "\n",
        "    def encoder(self, x):\n",
        "        out_encoded, (hidden_T, _) = self.LSTM_encoder(\n",
        "            x, (self.hidden_state, self.cell_state))\n",
        "\n",
        "        # Swap batch dimension to batch_first\n",
        "        hidden_T = hidden_T.transpose(0, 1)\n",
        "\n",
        "        # Flatten \n",
        "        flat_hidden_T = hidden_T.reshape(self.batch_size, 1, 2*self.h_dim)\n",
        "        \n",
        "        # Fully connected layer from LSTM to var and mu\n",
        "        mu_z = self.fnn_zmu(flat_hidden_T)\n",
        "        sigma_z = nn.functional.softplus(self.fnn_zvar(flat_hidden_T))\n",
        "\n",
        "        # Calculate the similarity matrix\n",
        "        S = torch.matmul(\n",
        "            out_encoded,\n",
        "            torch.transpose(out_encoded, 1, 2)\n",
        "        )\n",
        "        \n",
        "        S = S / np.sqrt((2 * self.h_dim))\n",
        "\n",
        "        # Use softmax to get the sum of weights to equal 1\n",
        "        A = nn.functional.softmax(S, dim=2)\n",
        "        Cdet = torch.matmul(A, out_encoded)\n",
        "\n",
        "        # Fully connected layer from LSTM to var and mu\n",
        "        mu_c = self.fnn_cmu(Cdet)\n",
        "        sigma_c = nn.functional.softplus(self.fnn_cvar(Cdet))\n",
        "\n",
        "        return mu_z, sigma_z, mu_c, sigma_c, A\n",
        "\n",
        "    def decoder(self, c, z):\n",
        "        # Concatenate z and c before giving it as input to the decoder\n",
        "        z_cat = torch.cat(self.T*[z], dim=1)\n",
        "        zc_concat = torch.cat((z_cat, c), dim=2)\n",
        "        \n",
        "        # Run through decoder\n",
        "        out_decoded, _ = self.LSTM_decoder(zc_concat)\n",
        "\n",
        "        # Pass the decoder outputs through fnn to get LaPlace parameters\n",
        "        mu_x = self.fnn_xmu(out_decoded)\n",
        "        b_x = nn.functional.softplus(self.fnn_xb(out_decoded))\n",
        "\n",
        "        return mu_x, b_x\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "          var_x = self.x_noise_factor*torch.var(x,dim=1) # (batch_size, x_dim)\n",
        "          noise = torch.randn(self.T, self.batch_size, self.x_dim)\n",
        "\n",
        "          # (48, 16, 1) * (16, 1)\n",
        "          if torch.cuda.is_available():\n",
        "            noise = noise.to(torch.device(0))\n",
        "\n",
        "          \n",
        "          x_noise = torch.mul(noise, torch.sqrt(var_x)) # (T, batch_size, x_dim)\n",
        "          x = x + torch.transpose(x_noise, 0, 1) # (batch_size, T, x_dim)\n",
        "\n",
        "\n",
        "        # TODO: Add monte-carlo integration\n",
        "        outputs = {}\n",
        "        \n",
        "        mu_z, sigma_z, mu_c, sigma_c, A = self.encoder(x)\n",
        "        \n",
        "        # Don't propagate gradients through randomness\n",
        "        with torch.no_grad():\n",
        "            epsilon_z = torch.randn(self.batch_size, 1, self.z_dim)\n",
        "            epsilon_c = torch.randn(self.batch_size, self.T, self.z_dim)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "          epsilon_z = epsilon_z.to(torch.device(0))\n",
        "          epsilon_c = epsilon_c.to(torch.device(0))\n",
        "\n",
        "        z = mu_z + epsilon_z * sigma_z\n",
        "        c = mu_c + epsilon_c * sigma_c\n",
        "\n",
        "        mu_x, b_x = self.decoder(c, z)\n",
        "\n",
        "        outputs[\"z\"] = z\n",
        "        outputs[\"mu_z\"] = mu_z\n",
        "        outputs[\"sigma_z\"] = sigma_z\n",
        "        outputs[\"c\"] = c\n",
        "        outputs[\"mu_c\"] = mu_c\n",
        "        outputs[\"sigma_c\"] = sigma_c\n",
        "        outputs[\"mu_x\"] = mu_x\n",
        "        outputs[\"b_x\"] = b_x\n",
        "        outputs[\"A\"] = A\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def count_parameters(self):\n",
        "        n_grad = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        n_total = sum(p.numel() for p in self.parameters())\n",
        "        return n_grad, n_total\n",
        "\n",
        "def ELBO_loss(x, outputs, lambda_KL=0.01, eta_KL=0.01):\n",
        "    mu_x = outputs['mu_x']\n",
        "    b_x = outputs['b_x']\n",
        "    mu_z = outputs[\"mu_z\"]\n",
        "    sigma_z = outputs[\"sigma_z\"]\n",
        "    mu_c = outputs[\"mu_c\"]\n",
        "    sigma_c = outputs[\"sigma_c\"]\n",
        "\n",
        "    # Initialize Laplace with given parameters.\n",
        "    pdf_laplace = torch.distributions.laplace.Laplace(mu_x, b_x)\n",
        "    # Calculate mean of likelihood over T and x-dimension.\n",
        "    likelihood = pdf_laplace.log_prob(x).mean(dim=1).mean(dim=1)\n",
        "\n",
        "    # Calculate KL-divergence of p(c) and q(z)\n",
        "    v_z = sigma_z**2  # Variance of q(z)\n",
        "    v_c = sigma_c**2  # Variance of p(c)\n",
        "    kl_z = -0.5 * torch.sum(1 + torch.log(v_z) - mu_z**2 - v_z, dim=2)\n",
        "    kl_c = -0.5 * torch.sum(1 + torch.log(v_c) - mu_c**2 - v_c, dim=2)\n",
        "\n",
        "    kl_c = torch.sum(kl_c, dim=1)  # Sum over the T dimension.\n",
        "    kl_z = kl_z[:, 0]  # Get rid of extra x_dim dimension.\n",
        "\n",
        "    # Calculate the Evidence Lower Bound (ELBO)\n",
        "    ELBO = -likelihood + lambda_KL * (kl_z + eta_KL * kl_c)\n",
        "\n",
        "    # Return mean over all examples in batch\n",
        "    # =================\n",
        "    # ====== $ ===== RETURN: SUM instead of MEAN?\n",
        "    # =================\n",
        "    return torch.mean(ELBO, dim=0)\n",
        "\n",
        "\n",
        "def similarity_score(net, x, L):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Pass sequence through encoder to get params in q(z) and p(c)\n",
        "        mu_z, sigma_z, mu_c, sigma_c, _ = net.encoder(x)\n",
        "        \n",
        "        score = 0\n",
        "\n",
        "        for _ in range(L):\n",
        "\n",
        "            # Sample a random c and z vector and reparametrize\n",
        "            epsilon_c = torch.randn(net.batch_size, net.T, net.z_dim)\n",
        "            epsilon_z = torch.randn(net.batch_size, 1, net.z_dim)\n",
        "            \n",
        "            if torch.cuda.is_available():\n",
        "                epsilon_c = epsilon_c.to(torch.device(0))\n",
        "                epsilon_z = epsilon_z.to(torch.device(0))\n",
        "\n",
        "            c = mu_c + epsilon_c * sigma_c\n",
        "            z = mu_z + epsilon_z * sigma_z\n",
        "\n",
        "            # Pass sample through decoder and calculate reconstruction prob\n",
        "            mu_x, b_x = net.decoder(c, z)\n",
        "            pdf_laplace = torch.distributions.laplace.Laplace(mu_x, b_x)\n",
        "            score += pdf_laplace.log_prob(x)\n",
        "\n",
        "        # Average over number of iterations\n",
        "        return score/L\n",
        "\n",
        "def get_sim_scores(net, dataset, L):\n",
        "\n",
        "    batch_size = min((len(dataset), dataset.T*100))\n",
        "\n",
        "    loader = DataLoader(dataset, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "    # Re-initialize network\n",
        "    net.batch_size = batch_size\n",
        "    net.init_hidden()\n",
        "    net.eval()\n",
        "    all_sim_scores = np.zeros((0, dataset.T))\n",
        "\n",
        "    for x, label in loader:\n",
        "        # Modify batch size and hidden state\n",
        "        if x.shape[0] != batch_size:\n",
        "            net.batch_size = x.shape[0]\n",
        "            net.init_hidden()\n",
        "\n",
        "        # Cast to gpu if available\n",
        "        if torch.cuda.is_available():\n",
        "            x = x.to(torch.device(0))\n",
        "        \n",
        "        temp_sim = tensor2numpy(similarity_score(net, x, L)).reshape(-1, dataset.T)\n",
        "        all_sim_scores = np.concatenate((all_sim_scores, temp_sim))\n",
        "\n",
        "    return all_sim_scores\n",
        "\n",
        "def get_z_values(net, dataset, batch_size):\n",
        "\n",
        "    # Set batch size to length of dataset    \n",
        "    net.batch_size = batch_size\n",
        "    net.init_hidden()\n",
        "    net.eval()\n",
        "\n",
        "    loader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
        "\n",
        "    z_all = np.zeros((0, net.z_dim))\n",
        "    with torch.no_grad():\n",
        "        for x, label in loader:\n",
        "            if torch.cuda.is_available():\n",
        "                x = x.to(torch.device(0))\n",
        "            \n",
        "            # Change batch size in last iteration\n",
        "            if not x.shape[0] == batch_size:\n",
        "                net.batch_size = x.shape[0]\n",
        "                net.init_hidden()\n",
        "                \n",
        "            output = net(x)\n",
        "            \n",
        "            # Extract z\n",
        "            z = output['z']\n",
        "            z_all = np.concatenate((z_all, tensor2numpy(z[:,0])))\n",
        "\n",
        "    # Restore batch_size\n",
        "    net.batch_size = batch_size\n",
        "    net.init_hidden()\n",
        "\n",
        "    return z_all\n",
        "\n",
        "def tensor2numpy(x):\n",
        "    if x.requires_grad:\n",
        "        x = torch.Tensor.cpu(x).detach().numpy()\n",
        "    else:\n",
        "        x = torch.Tensor.cpu(x).numpy()\n",
        "    return x\n",
        "\n",
        "def save_model(net, name):\n",
        "    _drive = \"/content/drive/My Drive/kongkat/models\"\n",
        "    f_name = name + \".pth\"\n",
        "    f_path = os.path.join(_drive, f_name)\n",
        "    torch.save(net.state_dict(), f_path)\n",
        "\n",
        "def load_model(parameters, name):\n",
        "    _drive = \"/content/drive/My Drive/kongkat/models\"\n",
        "    f_name = name + \".pth\"\n",
        "    f_path = os.path.join(_drive, f_name)\n",
        "    \n",
        "    model = VRASAM(*parameters)\n",
        "    model.load_state_dict(torch.load(f_path))\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.to(torch.device(0))\n",
        "    \n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwJp1J_r4_Bg",
        "colab_type": "text"
      },
      "source": [
        "# GENDATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iQZkBq-5Ek4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GENDATA_TRAIN(Dataset):\n",
        "\n",
        "    def __init__(self, N_seq, T_w):\n",
        "\n",
        "        datafolder = os.path.normpath(\"drive/My Drive/kongkat/data/GENDATA/\")\n",
        "        path = os.path.normpath(f\"{datafolder}/train_data.txt\")\n",
        "        self.data = np.genfromtxt(path)\n",
        "        self.N_seq = N_seq\n",
        "        self.T = 96\n",
        "        self.T_w = T_w\n",
        "        self.data = self.data[:(self.N_seq*self.T)] # Extract wanted number of sequences\n",
        "        self.X = torch.Tensor(self.data).view(-1, self.T_w)\n",
        "        self.time_labels = list(range(len(self.X)))\n",
        "        self.labels = [\"None\"]*N_seq\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        x = x.view(*x.shape, 1)\n",
        "        return x, self.time_labels[idx]\n",
        "\n",
        "    def get_window(self, idx):\n",
        "        x = self.X[idx]\n",
        "        x = x.view(1, -1, 1)\n",
        "        return x\n",
        "\n",
        "class GENDATA_VAL(Dataset):\n",
        "\n",
        "    def __init__(self, T_w):\n",
        "\n",
        "        datafolder = os.path.normpath(\"drive/My Drive/kongkat/data/GENDATA/\")\n",
        "        data_path = os.path.normpath(f\"{datafolder}/val_data.txt\")\n",
        "        anom_path = os.path.normpath(f\"{datafolder}/val_anoms.txt\")\n",
        "        label_path = os.path.normpath(f\"{datafolder}/val_labels.txt\")\n",
        "\n",
        "        # Skip first sequence\n",
        "        self.data = np.genfromtxt(data_path)[96:]\n",
        "        self.anom_labels = np.genfromtxt(anom_path)[1:]\n",
        "        self.labels = np.genfromtxt(label_path, dtype=\"|U5\")[1:]\n",
        "        self.T = 96\n",
        "        self.T_w = T_w\n",
        "        self.X = torch.Tensor(self.data).view(-1, self.T_w)\n",
        "        self.time_labels = list(range(len(self.X)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        x = x.view(*x.shape, 1)\n",
        "        return x, self.time_labels[idx]\n",
        "\n",
        "    def get_window(self, idx):\n",
        "        x = self.X[idx]\n",
        "        x = x.view(1, -1, 1)\n",
        "        return x\n",
        "\n",
        "class GENDATA_TEST(Dataset):\n",
        "\n",
        "    def __init__(self, T_w):\n",
        "\n",
        "        datafolder = os.path.normpath(\"drive/My Drive/kongkat/data/GENDATA/\")\n",
        "        data_path = os.path.normpath(f\"{datafolder}/test_data.txt\")\n",
        "        anom_path = os.path.normpath(f\"{datafolder}/test_anoms.txt\")\n",
        "        label_path = os.path.normpath(f\"{datafolder}/test_labels.txt\")\n",
        "        \n",
        "        # Skip first sequence\n",
        "        self.data = np.genfromtxt(data_path)[96:]\n",
        "        self.anom_labels = np.genfromtxt(anom_path)[1:]\n",
        "        self.labels = np.genfromtxt(label_path, dtype=\"|U5\")[1:]\n",
        "        self.T = 96\n",
        "        self.T_w = T_w\n",
        "        self.X = torch.Tensor(self.data).view(-1, self.T_w)\n",
        "        self.time_labels = list(range(len(self.X)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        x = x.view(*x.shape, 1)\n",
        "        return x, self.time_labels[idx]\n",
        "\n",
        "    def get_window(self, idx):\n",
        "        x = self.X[idx]\n",
        "        x = x.view(1, -1, 1)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXufJlB5DX6J",
        "colab_type": "text"
      },
      "source": [
        "# Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI5n8VUp7FwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train(dataset, net, optimizer, criterion, batch_size, lamba_kl=0.01, num_workers=0):\n",
        "    \n",
        "    # Set network into training mode\n",
        "    net.batch_size = batch_size\n",
        "    net.init_hidden()\n",
        "    net.train()\n",
        "    \n",
        "    loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "    for x, label in loader:\n",
        "        \n",
        "        # Cast to gpu if available\n",
        "        if torch.cuda.is_available():\n",
        "            x = x.to(torch.device(0))\n",
        "        \n",
        "        # Change batch size in last iteration\n",
        "        if not x.shape[0] == batch_size:\n",
        "            net.batch_size = x.shape[0]\n",
        "            net.init_hidden()\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Make forward pass with model\n",
        "        outputs = net(x)\n",
        "        # Calculate loss and backprop\n",
        "        loss = criterion(x, outputs, 0.01, lamba_kl)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Revert to correct net batch size\n",
        "    net.batch_size = batch_size\n",
        "    net.init_hidden()\n",
        "\n",
        "    return net\n",
        "\n",
        "def validate(dataset, net, criterion, batch_size, lambda_kl=0.01):\n",
        "    \n",
        "    # Set network into evalution mode\n",
        "    net.batch_size = batch_size\n",
        "    net.init_hidden()\n",
        "    net.eval()\n",
        "\n",
        "    loader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, label in loader:\n",
        "        \n",
        "        # Cast to gpu if available\n",
        "        if torch.cuda.is_available():\n",
        "            x = x.to(torch.device(0))\n",
        "\n",
        "        # Change batch size in last iteration\n",
        "        if not x.shape[0] == batch_size:\n",
        "            net.batch_size = x.shape[0]\n",
        "            net.init_hidden()\n",
        "\n",
        "        # Make forward pass with model\n",
        "        outputs = net(x)\n",
        "        loss = criterion(x, outputs, 0.01, lambda_kl)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Set model to train mode and revert batch size\n",
        "    net.batch_size = batch_size\n",
        "    net.init_hidden()\n",
        "    net.train()\n",
        "\n",
        "    return total_loss/len(loader)\n",
        "\n",
        "def train_validate(dataset, net, optimizer, criterion, n_epochs, batch_size, train_val_split=(0.8, 0.2), num_workers=0, verbose=True):\n",
        "\n",
        "    train_size = int(train_val_split[0] * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    lambda_kl = 0\n",
        "    inc_val = 2/n_epochs\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "\n",
        "        # Train model\n",
        "        net = train(train_set, net, optimizer, criterion, batch_size, lambda_kl, num_workers)\n",
        "\n",
        "        # Evaluate model on training data again and validate\n",
        "        with torch.no_grad():\n",
        "            train_loss = validate(train_set, net, criterion, batch_size, lambda_kl)\n",
        "            val_loss = validate(val_set, net, criterion, batch_size, lambda_kl)\n",
        "\n",
        "        if verbose:\n",
        "            print(\n",
        "                \"Epoch {0}/{1} done!\\tTrain loss = {2:.2f}\\tVal loss = {3:.2f}\".format(\n",
        "                    e+1, n_epochs, train_loss, val_loss)\n",
        "            )\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        lambda_kl += inc_val\n",
        "        \n",
        "    return net, (train_losses, val_losses)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaveotlaPaMP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Results plot functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwA2lXE8KpwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import torch\n",
        "\n",
        "def plot_sim(ax, x, samples, sim, min_sim, max_sim, title):\n",
        "\n",
        "    ax[0].plot(tensor2numpy(x[0, :, 0]), c='black')\n",
        "    ax[0].set_title(title)\n",
        "\n",
        "    # Plot regenerated normal data\n",
        "    ax[0].plot(samples, 'o', markersize=1, c='darksalmon', alpha=0.3, rasterized=True)\n",
        "    ax[0].set_xlim((0, 96))\n",
        "    ax[0].grid(False)\n",
        "    ax[0].set_xticks([])\n",
        "    ax[0].set_yticks([])\n",
        "\n",
        "    # Plot similarity\n",
        "    ax[1].imshow(tensor2numpy(sim).reshape(1, -1), aspect='auto', cmap='RdYlGn', vmin=min_sim, vmax=max_sim)\n",
        "    ax[1].grid(False)\n",
        "    ax[1].set_yticks([])\n",
        "    ax[1].set_xticks([])\n",
        "\n",
        "    return ax\n",
        "\n",
        "def results_plot(ax1, ax2, net, all_sim_scores, L=1, n_samples=10):\n",
        "\n",
        "    outlier_types = [\"Snow\", \"Fault\", \"Spike\", \"Shade\", \"Cloud\"]\n",
        "\n",
        "    # Get min and max similarity score\n",
        "    min_sim = np.quantile(all_sim_scores, 0.02)\n",
        "    max_sim = np.quantile(all_sim_scores, 0.9)\n",
        "\n",
        "    # Load test data\n",
        "    test_data = GENDATA_TEST(T_w)\n",
        "    X = test_data.X.reshape(-1, test_data.T_w)\n",
        "    labels = test_data.labels\n",
        "\n",
        "    # Extract random normal sequence\n",
        "    normal_X = X[labels == \"None\"]\n",
        "    x_normal = random.choice(normal_X).view(1, -1, 1)\n",
        "\n",
        "    # Allocate outliers array\n",
        "    X_outliers = torch.Tensor(np.zeros((len(outlier_types), *(x_normal.shape))))\n",
        "\n",
        "    # Extract specified or random outlier sequence\n",
        "    for i, out_type in enumerate(outlier_types):\n",
        "        outlier_idxs = np.arange(X.shape[0])[labels == out_type]\n",
        "        rand_outlier_idx = random.choice(outlier_idxs)\n",
        "        X_outliers[i] = X[rand_outlier_idx].view(1, -1, 1)\n",
        "\n",
        "    # Cast to GPU\n",
        "    if torch.cuda.is_available():\n",
        "        x_normal = x_normal.to(torch.device(0))\n",
        "        X_outliers = X_outliers.to(torch.device(0))\n",
        "    \n",
        "    # Reshape outliers data\n",
        "    X_outliers = X_outliers.view(-1, test_data.T_w, 1)\n",
        "\n",
        "    # Create regeneration\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Set network batch size down and reinitalize hidden- and cell state\n",
        "        net.batch_size = x_normal.shape[0]\n",
        "        net.init_hidden()\n",
        "\n",
        "        # Allocate arrays for normal samples\n",
        "        normal_samples = np.zeros((x_normal.shape[1], n_samples))\n",
        "\n",
        "        # Calculate similarity\n",
        "        sim_normal = similarity_score(net, x_normal, L)[0, :, 0]\n",
        "\n",
        "        # Pass sequences through encoder to get params in q(z) and p(c)\n",
        "        mu_z_normal, sigma_z_normal, mu_c_normal, sigma_c_normal, _ = net.encoder(x_normal)\n",
        "        \n",
        "        # Sample n_samples times\n",
        "        for i in range(n_samples):\n",
        "            # Sample a random c and z vector and reparametrize\n",
        "            epsilon_c_normal = torch.randn(net.batch_size, net.T, net.z_dim)\n",
        "            epsilon_z_normal = torch.randn(net.batch_size, 1, net.z_dim)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                epsilon_c_normal = epsilon_c_normal.to(torch.device(0))\n",
        "                epsilon_z_normal = epsilon_z_normal.to(torch.device(0))\n",
        "\n",
        "            c_normal = mu_c_normal + epsilon_c_normal * sigma_c_normal\n",
        "            z_normal = mu_z_normal + epsilon_z_normal * sigma_z_normal\n",
        "\n",
        "            # Pass sample through decoder and calculate reconstruction prob\n",
        "            mu_x_normal, b_x_normal = net.decoder(c_normal, z_normal)\n",
        "            normal_samples[:, i] = tensor2numpy(mu_x_normal[0, :, 0])\n",
        "\n",
        "        # Set network batch size down and reinitalize hidden- and cell state\n",
        "        net.batch_size = X_outliers.shape[0]\n",
        "        net.init_hidden()\n",
        "\n",
        "        # Allocate arrays for outlier samples\n",
        "        outlier_samples = np.zeros((len(outlier_types), test_data.T_w,  n_samples))\n",
        "\n",
        "        sim_outlier = similarity_score(net, X_outliers, L=L)[:, :, 0]\n",
        "        mu_z_outlier, sigma_z_outlier, mu_c_outlier, sigma_c_outlier, _ = net.encoder(X_outliers)        \n",
        "\n",
        "        # Sample n_samples times\n",
        "        for i in range(n_samples):\n",
        "\n",
        "            epsilon_c_outlier = torch.randn(net.batch_size, net.T, net.z_dim)\n",
        "            epsilon_z_outlier = torch.randn(net.batch_size, 1, net.z_dim)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                epsilon_c_outlier = epsilon_c_outlier.to(torch.device(0))\n",
        "                epsilon_z_outlier = epsilon_z_outlier.to(torch.device(0))\n",
        "\n",
        "            c_outlier = mu_c_outlier + epsilon_c_outlier * sigma_c_outlier\n",
        "            z_outlier = mu_z_outlier + epsilon_z_outlier * sigma_z_outlier\n",
        "\n",
        "            mu_x_outlier, b_x_outlier = net.decoder(c_outlier, z_outlier)\n",
        "            outlier_samples[:, :, i] = tensor2numpy(mu_x_outlier[:, :, 0])\n",
        "\n",
        "    # Reshape outlier similarity and samples\n",
        "    outlier_samples = outlier_samples.reshape(len(outlier_types), -1, n_samples)\n",
        "    sim_outlier = sim_outlier.view(len(outlier_types), -1, 1)\n",
        "    X_outliers = X_outliers.view(len(outlier_types), -1, test_data.T_w, 1)\n",
        "\n",
        "    # Define range of plotting\n",
        "    plot_lim = (-0.1, 1.1)\n",
        "\n",
        "    ## Plot outlier data ##\n",
        "    outlier_axes = [ax1[:, 1], ax1[:, 2], ax2[:, 0], ax2[:, 1], ax2[:, 2]]\n",
        "    for i in range(len(outlier_axes)):\n",
        "        plot_sim(outlier_axes[i], X_outliers[i], outlier_samples[i], sim_outlier[i], min_sim, max_sim, f'{outlier_types[i].capitalize()}')\n",
        "        outlier_axes[i][0].set_ylim(plot_lim)\n",
        "\n",
        "    ## Plot normal data ##\n",
        "    plot_sim(ax1[:, 0], x_normal, normal_samples, sim_normal, min_sim, max_sim, 'Normal data')\n",
        "    ax1[0, 0].set_ylim(plot_lim)\n",
        "    ax1[0, 0].set_ylabel('Energy', fontsize=14)\n",
        "    ax2[0, 0].set_ylabel('Energy', fontsize=14)\n",
        "    ax1[0, 0].set_yticks([0, 1])\n",
        "    ax2[0, 0].set_yticks([0, 1])\n",
        "    ax1[0, 0].yaxis.set_tick_params(labelsize=14)\n",
        "    ax2[0, 0].yaxis.set_tick_params(labelsize=14)\n",
        "\n",
        "    return ax1, ax2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7AJBFGqIg6o",
        "colab_type": "text"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VafiFT8N6Uj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Set seed mu\n",
        "seed = 7\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "N = 1400  # Number of sequences\n",
        "T = 96  # Length of sequences\n",
        "T_w = 96  # Window length\n",
        "batch_size = 200\n",
        "train_val_split = (0.8, 0.2)\n",
        "train_data = GENDATA_TRAIN(N, T_w)\n",
        "\n",
        "# Model hyperparameters\n",
        "n_epochs = 1000\n",
        "z_dim = 3\n",
        "h_dim = 128  # Number of LSTM units in each direction\n",
        "x_dim = 1\n",
        "lr = 0.001\n",
        "criterion = ELBO_loss\n",
        "parameters = [z_dim, T_w, x_dim, h_dim, batch_size]\n",
        "net = VRASAM(*parameters)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr, amsgrad=True)\n",
        "\n",
        "# Train model\n",
        "if torch.cuda.is_available():\n",
        "    net = net.to(torch.device(0))\n",
        "\n",
        "pars = {'dataset': train_data, \n",
        "        'net': net,\n",
        "        'optimizer': optimizer,\n",
        "        'criterion': criterion,\n",
        "        'n_epochs': n_epochs,\n",
        "        'batch_size': batch_size,\n",
        "        'train_val_split': train_val_split,\n",
        "        'num_workers': 0,\n",
        "        'verbose': True}\n",
        "\n",
        "net, (train_losses, val_losses) = train_validate(**pars)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6UVk1l-C52F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load test data and calculate sim score\n",
        "test_data = GENDATA_TEST(T_w)\n",
        "test_all_sim_scores = get_sim_scores(net, test_data, L=512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVNwrtMqC9OS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig1, ax1 = plt.subplots(2, 3, \n",
        "                       gridspec_kw={\n",
        "                           'width_ratios': [1, 1, 1],\n",
        "                           'height_ratios': [5, 1]},\n",
        "                       figsize=(10, 2))\n",
        "fig1.subplots_adjust(hspace=0, wspace=0.05)\n",
        "\n",
        "fig2, ax2 = plt.subplots(2, 3, \n",
        "                       gridspec_kw={\n",
        "                           'width_ratios': [1, 1, 1],\n",
        "                           'height_ratios': [5, 1]},\n",
        "                       figsize=(10, 2))\n",
        "fig2.subplots_adjust(hspace=0, wspace=0.05)\n",
        "\n",
        "# Visualize the reconstruction results on normal and outlier series\n",
        "ax1 = results_plot(ax1, ax2, net, test_all_sim_scores, L=512, n_samples=50)\n",
        "fig1.savefig(kongpath+'figures/regen_plot_offline1.pdf', format='pdf')\n",
        "fig2.savefig(kongpath+'figures/regen_plot_offline2.pdf', format='pdf')\n",
        "plt.show()\n",
        "\n",
        "#fig, ax = plt.subplots(1,1,figsize=(10, 3))\n",
        "#ax.plot(train_losses, label='Training loss', c='darkslategrey')\n",
        "#ax.plot(val_losses, label='Validation loss', c='darksalmon')\n",
        "#ax.set_xlabel('Epochs', fontsize=14)\n",
        "#ax.set_ylabel('Loss', fontsize=14)\n",
        "#plt.legend(fontsize=14)\n",
        "#plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aawoz37CGr-9",
        "colab_type": "text"
      },
      "source": [
        "# Model visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXSvpQrtGvyW",
        "colab_type": "text"
      },
      "source": [
        "## Outlier PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8pylNDIGt-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "# Process test data\n",
        "test_labels = test_data.labels\n",
        "\n",
        "# Extract all z_values for training data and standardize\n",
        "z_all = get_z_values(net, test_data, batch_size=batch_size)\n",
        "#z_all2 = get_z_values(net, train_data, batch_size=batch_size)\n",
        "#z_all = np.concatenate((z_all1, z_all2))\n",
        "#all_labels = np.concatenate((train_data.labels, test_labels))\n",
        "\n",
        "z_all = StandardScaler().fit_transform(z_all)\n",
        "\n",
        "pca = PCA()\n",
        "principal_comps = pca.fit_transform(z_all)\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwBdpZC7HNXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.cm as cm\n",
        "\n",
        "# Mapping ditionary\n",
        "outlier_types = [\"None\", \"Snow\", \"Fault\", \"Spike\", \"Shade\", \"Cloud\"]\n",
        "colors = ['black', 'darksalmon', 'springgreen', 'skyblue', 'fuchsia', 'orangered']\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(7,4))\n",
        "for i,out_type in enumerate(outlier_types):\n",
        "    idxs = test_labels == out_type\n",
        "    ax.scatter(principal_comps[:,0][idxs], principal_comps[:,1][idxs], s=20, color=colors[i])\n",
        "\n",
        "custom_lines = [Line2D([0], [0], linestyle='', marker='o', color=col, markersize=7) for col in colors]\n",
        "\n",
        "outlier_types[0] = \"Normal\"\n",
        "(x1, x2) = ax.get_xlim()\n",
        "x2 *= 1.18\n",
        "ax.set_xlim(x1, x2)\n",
        "ax.legend(custom_lines, outlier_types)\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "plt.tight_layout()\n",
        "plt.savefig('drive/My Drive/kongkat/figures/salami_pca.pdf', format='pdf')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znaNwPD4LvOh",
        "colab_type": "text"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MuPwCGBHPqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "def get_roc(ytrue, sim_scores):\n",
        "    #Extract propriate values for roc-curve\n",
        "    ytrue_roc = []\n",
        "    for i in range(len(ytrue)):\n",
        "      if ytrue[i] == 0:\n",
        "        ytrue_roc.append(1)\n",
        "      else:\n",
        "        ytrue_roc.append(0)\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(ytrue_roc, sim_scores)\n",
        "    return fpr, tpr, thresholds\n",
        "\n",
        "def plotRocCurve(ax, ytrue, sim_scores):\n",
        "\n",
        "    #Extract propriate values for roc-curve\n",
        "    ytrue_roc = []\n",
        "    for i in range(len(ytrue)):\n",
        "      if ytrue[i] == 0:\n",
        "        ytrue_roc.append(1)\n",
        "      else:\n",
        "        ytrue_roc.append(0)\n",
        "        \n",
        "    fpr, tpr, thresholds = get_roc(ytrue, sim_scores)\n",
        "\n",
        "    auc = roc_auc_score(ytrue_roc, sim_scores)\n",
        "    random = [0,1]\n",
        "\n",
        "    # plot the roc curve for the model\n",
        "    ax.plot(fpr, tpr, linestyle='--', label=f'Threshold prediction (AUC={auc:.2f})', c='darkslategrey')\n",
        "    ax.plot(random, random, linestyle='--', label='No information', c='darksalmon')\n",
        "    # axis labels\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.tick_params(axis='both', which='major')\n",
        "    ax.set_xticks([0, 0.5, 1])\n",
        "    ax.set_yticks([0, 0.5, 1])\n",
        "    # show the legend\n",
        "    ax.legend()\n",
        "\n",
        "    return thresholds, auc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBLdXzzFvnQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load threshold data\n",
        "val_data = GENDATA_VAL(96)\n",
        "val_all_sim_scores = get_sim_scores(net, val_data, L=512)\n",
        "val_sim_scores = val_all_sim_scores.sum(axis=1)\n",
        "val_ytrue = val_data.labels != 'None'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krzrVDjRMFRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Choose optimal threshold\n",
        "_, _, thresholds = get_roc(val_ytrue, val_sim_scores)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(15,5))\n",
        "acc = np.zeros(len(thresholds))\n",
        "\n",
        "for i,thres in enumerate(thresholds):\n",
        "    val_ypred = val_sim_scores < thres\n",
        "    trues = sum(val_ypred == val_ytrue)\n",
        "    total = len(val_ypred)\n",
        "    acc[i] = trues/total\n",
        "\n",
        "ax.bar(range(len(thresholds)), acc, color='darkslategrey')\n",
        "ax.set_xticks([])\n",
        "ax.set_xlabel('Reconstruction likelihood thresholds', fontsize=16)\n",
        "ax.set_ylabel('Accuracy', fontsize=16)\n",
        "for index, value in enumerate(acc):\n",
        "    plt.text(index, value-0.05, f'{value*100:.0f}', ha='center', color='white', fontsize=10)\n",
        "plt.show()\n",
        "\n",
        "# Extract best threshold\n",
        "threshold = thresholds[np.argmax(acc)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWz4SQj3Mau_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "\n",
        "# Process final test data\n",
        "test_sim_scores = test_all_sim_scores.sum(axis=1)\n",
        "test_labels = test_data.labels\n",
        "test_ytrue = test_labels != \"None\"\n",
        "\n",
        "# Plot ROC curve\n",
        "fig, ax = plt.subplots(1, 1, figsize=(5,3))\n",
        "thresholds, auc = plotRocCurve(ax, test_ytrue, test_sim_scores)\n",
        "plt.savefig(kongpath+'figures/ROC_offline.pdf', format='pdf')\n",
        "plt.show()\n",
        "\n",
        "# Plot confusion matrix\n",
        "class_names = [\"None\", \"Snow\", \"Fault\", \"Spike\", \"Shade\", \"Cloud\"]\n",
        "\n",
        "test_ypred = test_sim_scores < threshold\n",
        "correct = test_ytrue == test_ypred\n",
        "conf_mat = np.zeros((2, len(class_names)))\n",
        "\n",
        "total_dict = Counter(test_labels)\n",
        "\n",
        "for i,cls in enumerate(class_names):\n",
        "    acc_cls = sum(correct[test_labels == cls])/total_dict[cls]\n",
        "    if cls == \"None\":\n",
        "        conf_mat[0,i] = acc_cls\n",
        "        conf_mat[1,i] = 1 - acc_cls\n",
        "    else:\n",
        "        conf_mat[0,i] = 1 - acc_cls\n",
        "        conf_mat[1,i] = acc_cls\n",
        "\n",
        "class_names = [\"Normal\", \"Snow\", \"Fault\", \"Spike\", \"Shade\", \"Cloud\"]\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6,2))\n",
        "df_cm = pd.DataFrame(conf_mat, ['Normal', 'Outlier'], class_names)\n",
        "sns.set(font_scale=1.4)#for label size\n",
        "sns.heatmap(df_cm, fmt='.2f', annot=True, annot_kws={\"size\": 10}, ax=ax, cmap='Greys', cbar=False)# font size\n",
        "\n",
        "ax.set_yticklabels(['Normal', 'Outlier'], va='center')\n",
        "ax.set_xlabel(\"True value\", fontsize=12)\n",
        "ax.set_ylabel(\"Prediction\", fontsize=12)\n",
        "\n",
        "plt.savefig(kongpath+'figures/confusion_matrix_offline.pdf', format='pdf')\n",
        "plt.show()\n",
        "mpl.rcParams.update(mpl.rcParamsDefault)\n",
        "print(f'Final accuracy: {sum(correct)/len(correct):.2f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjXnMInNgVcl",
        "colab_type": "text"
      },
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwckLyQKt9-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GENDATA visualization\n",
        "fig, axes = plt.subplots(4, 3, figsize=(10,5))\n",
        "#fig.subplots_adjust(hspace=0.2, wspace=0.05)\n",
        "\n",
        "outlier_types = [\"None\", \"Snow\", \"Fault\", \"Spike\", \"Shade\", \"Cloud\"]\n",
        "X = test_data.X.reshape(-1, test_data.T_w)\n",
        "labels = test_data.labels\n",
        "anoms = test_data.anom_labels\n",
        "\n",
        "col2label = {\"None\": 'black',\n",
        "             \"Snow\": 'darksalmon', \n",
        "             \"Fault\": 'springgreen', \n",
        "             \"Spike\": 'skyblue', \n",
        "             \"Shade\": 'fuchsia', \n",
        "             \"Cloud\": 'orangered'}\n",
        "\n",
        "net.batch_size = 1\n",
        "net.init_hidden()\n",
        "net.eval()\n",
        "\n",
        "for i, (ax, out_type) in enumerate(zip(axes.T.flatten()[::2], outlier_types)):\n",
        "    outlier_idxs = np.arange(X.shape[0])[labels == out_type]\n",
        "    idx = random.choice(outlier_idxs)\n",
        "    x = X[idx]\n",
        "    if out_type == \"None\":\n",
        "        ax.plot(x, c=col2label[\"None\"])\n",
        "        ax.set_title(f'{out_type}')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_ylim(-0.1, 1.1)\n",
        "    else:\n",
        "        all_idxs = np.arange(96)\n",
        "        anom_start, anom_end = anoms[idx]\n",
        "        out_idxs = (all_idxs<=anom_end)*(all_idxs>=anom_start)\n",
        "        norm_idxs1 = all_idxs<=anom_start\n",
        "        norm_idxs2 = all_idxs>=anom_end\n",
        "        ax.plot(all_idxs[norm_idxs1], x[norm_idxs1], c=col2label[\"None\"])\n",
        "        ax.plot(all_idxs[norm_idxs2], x[norm_idxs2], c=col2label[\"None\"])\n",
        "        ax.plot(all_idxs[out_idxs], x[out_idxs], c=col2label[out_type])\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_ylim(-0.1, 1.1)\n",
        "    \n",
        "    x = torch.Tensor(x).to(torch.device(0)).view(1, -1, 1)\n",
        "    attention = torch.log(net(x)['A'])\n",
        "    ax_i, ax_j = divmod(i, 3)\n",
        "    ax_i = (ax_i+1)*2-1\n",
        "    axes[ax_i, ax_j].imshow(tensor2numpy(attention[0]), cmap='Reds')\n",
        "\n",
        "\n",
        "    \n",
        "axes[0,0].set_yticks([0, 1])\n",
        "axes[1,0].set_yticks([0, 1])\n",
        "axes[0,0].set_ylabel('Energy')\n",
        "axes[1,0].set_ylabel('Energy')\n",
        "axes[1,1].set_xticks([0, 48, 96])\n",
        "axes[1,1].set_xlabel('Time of day')\n",
        "plt.savefig(kongpath+'figures/gendata_examples.pdf', format='pdf')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}